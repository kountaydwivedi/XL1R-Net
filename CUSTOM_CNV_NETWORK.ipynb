{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PHD.cnv_model_utils as u\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "seed = u.get_seed()\n",
    "print('Seed = ', seed)\n",
    "u.set_all_seeds(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"D:/CANCER BIOLOGY/DATASET/CNV/TCGA/FROM cBioPortal/\"\n",
    "PATH = 'D:/CANCER BIOLOGY/DATASET/CNV/TCGA/FROM Xena/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = pd.read_csv(PATH+'CNV_CBIOPORTAL_DATA_PREPROCESSED.gz', sep='\\t', compression='gzip')\n",
    "df_final = pd.read_csv(PATH+'TCGA_XENA_LUAD_LUSC_CNV_dataset_preprocessed.gz', sep='\\t', compression='gzip')\n",
    "df_final = df_final.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "labels = list(df_final['label'])\n",
    "df_final.drop(columns=['label'], axis=1, inplace=True) ## drop column sample_id and label\n",
    "columns = list(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = df_final.to_numpy()\n",
    "ytrain = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if  torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNV(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        print(\"No. of samples in dataset = \", len(self.X))\n",
    "        print(\"No. of unique labels in dataset = \", set(self.y))\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, exe='lasso'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.exe = exe\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, 4096)\n",
    "        self.norm1 = nn.Dropout(0.4)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.layer2 = nn.Linear(4096, 2048)\n",
    "        self.norm2 = nn.Dropout(0.3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.layer3 = nn.Linear(2048, 1024)\n",
    "        self.norm3 = nn.Dropout(0.2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.layer4 = nn.Linear(1024, 512)\n",
    "        self.norm4 = nn.Dropout(0.1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.layer5 = nn.Linear(512, output_dim)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        self.layer6 = nn.Linear(output_dim, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        importances = []\n",
    "        \n",
    "        layer1 = self.layer1(x)\n",
    "        norm1 = self.norm1(layer1)\n",
    "        relu1 = self.relu1(norm1)\n",
    "        importances.append(compute_lasso(relu1, y) if self.exe=='lasso' else compute_xgb(relu1, y))\n",
    "        \n",
    "        layer2 = self.layer2(relu1)\n",
    "        norm2 = self.norm2(layer2)\n",
    "        relu2 = self.relu2(norm2)\n",
    "        importances.append(compute_lasso(relu2, y) if self.exe=='lasso' else compute_xgb(relu2, y))\n",
    "        \n",
    "        layer3 = self.layer3(relu2)\n",
    "        norm3 = self.norm3(layer3)\n",
    "        relu3 = self.relu3(norm3)\n",
    "        importances.append(compute_lasso(relu3, y) if self.exe=='lasso' else compute_xgb(relu3, y))\n",
    "        \n",
    "        layer4 = self.layer4(relu3)\n",
    "        norm4 = self.norm4(layer4)\n",
    "        relu4 = self.relu4(norm4)\n",
    "        importances.append(compute_lasso(relu4, y) if self.exe=='lasso' else compute_xgb(relu4, y))\n",
    "        \n",
    "        layer5 = self.layer5(relu4)\n",
    "        relu5 = self.relu5(layer5)\n",
    "        importances.append(compute_lasso(relu5, y) if self.exe=='lasso' else compute_xgb(relu5, y))\n",
    "        \n",
    "        layer6 = self.layer6(relu5)\n",
    "        sigmoid = self.sigmoid(layer6)\n",
    "        importances.append(compute_lasso(sigmoid, y) if self.exe=='lasso' else compute_xgb(sigmoid, y))\n",
    "        \n",
    "        return sigmoid, importances\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Utility functions:\n",
    "\n",
    "1. compute_lasso()\n",
    "2. compute_xgb()\n",
    "3. nudge_weights()\n",
    "4. train_model()\n",
    "5. validate_model()\n",
    "\n",
    "'''\n",
    "\n",
    "## effect of alpha in LASSO regularization\n",
    "## https://chrisalbon.com/code/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression/\n",
    "def compute_lasso(samples, labels):\n",
    "    c_lasso = Lasso(alpha=0.0001, random_state=seed)\n",
    "    c_lasso.fit(samples.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
    "    return c_lasso.coef_\n",
    "\n",
    "def compute_xgb(samples, labels):\n",
    "    c_xgb = XGBClassifier(max_depth=100, random_state=seed, verbosity=0)\n",
    "    c_xgb.fit(samples.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
    "    return c_xgb.feature_importances_\n",
    "\n",
    "def nudge_weights(model, importances):\n",
    "    i = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            important_tensor = torch.tensor([importances[i]]).T.to(device)\n",
    "            min_val = torch.min(param.grad)\n",
    "            i+=1\n",
    "            if min_val != 0:\n",
    "                updation = torch.log(torch.abs(min_val)).to(device)\n",
    "                important_tensor*=(10**updation)\n",
    "                param.grad += important_tensor\n",
    "    \n",
    "def train_model(device, loader, model, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    for samples, labels in loader:\n",
    "        samples = samples.to(device)\n",
    "        samples = samples.float()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        ## ---------------------------------------------------------------------------- ##\n",
    "        \n",
    "        optimizer.zero_grad()  ## zer the previously computed gradients and start afresh\n",
    "        preds, importances = model(samples, labels)  ## train the model\n",
    "        cost = loss_fn(preds, labels)  ## compute cost\n",
    "        cost.backward()  ## compute gradients\n",
    "        nudge_weights(model, importances)  ## update the weights using computed importances\n",
    "        optimizer.step()  ## perform a single step of optimizer to update the weights\n",
    "        \n",
    "        ## ---------------------------------------------------------------------------- ##\n",
    "        \n",
    "        scores, predictions = torch.max(preds, 1)\n",
    "        return (predictions == labels).sum().item(), cost.item()\n",
    "    \n",
    "\n",
    "def validate_model(device, loader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for samples, labels in loader:\n",
    "            samples = samples.to(device)\n",
    "            samples = samples.float()\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds, _ = model(samples, labels)\n",
    "\n",
    "            scores, predictions = torch.max(preds, 1)\n",
    "            fpr, tpr, _ = roc_curve(labels.detach().cpu(), predictions.detach().cpu())\n",
    "            auroc = roc_auc_score(labels.detach().cpu(), predictions.detach().cpu())\n",
    "            auprc = average_precision_score(labels.detach().cpu(), predictions.detach().cpu())\n",
    "            precision, recall, _ = precision_recall_curve(labels.detach().cpu(), predictions.detach().cpu())\n",
    "            return (predictions == labels).sum().item(), fpr, tpr, precision, recall, auroc, auprc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_dim = xtrain.shape[1]\n",
    "output_dim = 256\n",
    "epochs = 50\n",
    "learn = 1e-4\n",
    "dataset = CNV(df_final.to_numpy(), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "1. EXECUTION USING LASSO REGRESSION\n",
    "\n",
    "Perform K-Fold cross validation using SubsetSampler\n",
    "feature of PyTorch. Try to follow this approach from now on.\n",
    "Essential links:\n",
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md\n",
    "https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f\n",
    "'''\n",
    "\n",
    "mean_train_score = []\n",
    "mean_fpr = []\n",
    "mean_tpr = []\n",
    "mean_precision = []\n",
    "mean_recall = []\n",
    "mean_auroc = []\n",
    "mean_auprc = []\n",
    "\n",
    "mean_valid_score = []\n",
    "k = 10\n",
    "u.set_all_seeds(seed)\n",
    "kfold = StratifiedKFold(n_splits=k, random_state=seed, shuffle=True)\n",
    "\n",
    "## start the k-fold\n",
    "for train_ids, valid_ids in kfold.split(dataset, labels):\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_ids)\n",
    "    valid_sampler = SubsetRandomSampler(valid_ids)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "    \n",
    "    u.set_all_seeds(seed)\n",
    "    model = Network(input_dim, output_dim, exe = 'lasso')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ## train and validate\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        train_correct, final_loss = train_model(device, train_loader, model, optimizer, loss_fn)\n",
    "        valid_correct, fpr, tpr, precision, recall, auroc, auprc = validate_model(device, valid_loader, model)\n",
    "        if epoch % 20 == 0:\n",
    "            print(\"Epoch: %03d/%03d | Train Acc %.3f | Valid Acc %.3f | Loss %.3f\" %\n",
    "                  (epoch, epochs, train_correct/batch_size, valid_correct/batch_size, final_loss))\n",
    "    \n",
    "    print('Time of exe:', time.time()-start_time)\n",
    "    print('\\n*******************\\n')\n",
    "    \n",
    "    mean_train_score.append(train_correct/batch_size)\n",
    "    mean_valid_score.append(valid_correct/batch_size)\n",
    "    mean_fpr.append(fpr)\n",
    "    mean_tpr.append(tpr)\n",
    "    mean_precision.append(precision)\n",
    "    mean_recall.append(recall)\n",
    "    mean_auroc.append(auroc)\n",
    "    mean_auprc.append(auprc)\n",
    "\n",
    "print('MEAN_TRAIN_SCORE : %.3f | MEAN_VALID_SCORE : %.3f'%(np.mean(mean_train_score), np.mean(mean_valid_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mean_auroc), np.mean(mean_auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUROC and AUPRC curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# AUROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='b', lw=2, label=f'Mean AUROC = {np.mean(mean_auroc):.3f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (AUROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('auroc_cnv.pdf', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# AUPRC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='b', lw=2, label=f'Mean AUPRC = {np.mean(mean_auprc):.3f}')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (AUPRC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig('auprc_cnv.pdf', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH+\"models/CUSTOM_CNV_NETWORK_lasso.kd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "2. EXECUTION USING XGBOOST\n",
    "\n",
    "Perform K-Fold cross validation using SubsetSampler\n",
    "feature of PyTorch. Try to follow this approach from now on.\n",
    "Essential links:\n",
    "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md\n",
    "https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f\n",
    "'''\n",
    "\n",
    "mean_train_score = []\n",
    "mean_auroc = []\n",
    "mean_auprc = []\n",
    "mean_valid_score = []\n",
    "k = 10\n",
    "u.set_all_seeds(seed)\n",
    "kfold = StratifiedKFold(n_splits=k, random_state=seed, shuffle=True)\n",
    "\n",
    "## start the k-fold\n",
    "for train_ids, valid_ids in kfold.split(dataset, labels):\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_ids)\n",
    "    valid_sampler = SubsetRandomSampler(valid_ids)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "    \n",
    "    u.set_all_seeds(seed)\n",
    "    model = Network(input_dim, output_dim, exe = 'xgb')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    ## train and validate\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        train_correct, final_loss = train_model(device, train_loader, model, optimizer, loss_fn)\n",
    "        valid_correct, auroc, auprc = validate_model(device, valid_loader, model)\n",
    "        if epoch % 20 == 0:\n",
    "            print(\"Epoch: %03d/%03d | Train Acc %.3f | Valid Acc %.3f | Loss %.3f\" %\n",
    "                  (epoch, epochs, train_correct/batch_size, valid_correct/batch_size, final_loss))\n",
    "    \n",
    "    print('Time of exe:', time.time()-start_time)\n",
    "    print('\\n*******************\\n')\n",
    "    \n",
    "    mean_train_score.append(train_correct/batch_size)\n",
    "    mean_valid_score.append(valid_correct/batch_size)\n",
    "\n",
    "print('MEAN_TRAIN_SCORE : %.3f | MEAN_VALID_SCORE : %.3f'%(np.mean(mean_train_score), np.mean(mean_valid_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PATH+\"models/CUSTOM_CNV_NETWORK_xgb.kd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "\n",
    "fs, data = read('PHD/alert.wav', mmap=True)  # fs - sampling frequency\n",
    "data = data.reshape(-1, 1)\n",
    "import sounddevice as sd\n",
    "sd.play(data, 44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
